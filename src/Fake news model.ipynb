{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random   \n",
    "import pickle\n",
    "import spacy\n",
    "import csv\n",
    "\n",
    "\n",
    "# Files and folders paths\n",
    "ABSOLUTE_PATH = './'\n",
    "DATASET_PATH = ABSOLUTE_PATH + './datasets/'\n",
    "X_pkl_file = ABSOLUTE_PATH + './X.pkl'\n",
    "TESTS_FILENAME = ABSOLUTE_PATH + './results/tests.csv'\n",
    "\n",
    "# Number of features to be extracted from the embedding\n",
    "NUM_FEATURES_VECTOR = 300\n",
    "\n",
    "# Number of words\n",
    "VOCABULARY_SIZE = 5000\n",
    "\n",
    "# Number of words in a text (more will be cut, less will be padded)\n",
    "WORDS_IN_SENTENCE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(documents, features=['percentage_punctuation', 'percentage_uppercase']):\n",
    "    \"\"\"\n",
    "    Returns array of features to pass as input to the model\n",
    "    \n",
    "    :param documents: list of string\n",
    "    \n",
    "    :return: list of features\n",
    "    \"\"\"    \n",
    "    output = []\n",
    "    for document in documents:\n",
    "        document_features = []\n",
    "        \n",
    "        # Total number of characters to compute percentage\n",
    "        num_chars = len(document)\n",
    "        \n",
    "        if 'percentage_punctuation' in features:\n",
    "            # Lambda function to count characters in string\n",
    "            count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "            num_punct = count(document, set(string.punctuation))\n",
    "            percentage_punct = num_punct / num_chars if num_chars != 0 else 0\n",
    "            document_features.append(percentage_punct)\n",
    "        \n",
    "        if 'percentage_uppercase' in features:\n",
    "            num_upper = sum(map(str.isupper, document))\n",
    "            percentage_upper = num_upper / num_chars if num_chars != 0 else 0\n",
    "            document_features.append(percentage_upper)\n",
    "        \n",
    "        output.append(document_features)\n",
    "    \n",
    "    return pd.DataFrame(output, columns=features)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 800\n",
      "Validation size: 100\n",
      "Test size: 100\n",
      "Number of documents for each class: {1: 430, 0: 570}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "df = pd.read_csv(DATASET_PATH + 'train.csv')\n",
    "df.dropna(how=\"any\", inplace = True)\n",
    "\n",
    "# Take the first 1000 only\n",
    "X_ = df['text'][:1000]\n",
    "y = df['label'][:1000]\n",
    "\n",
    "documents_per_class = {\n",
    "    label: y[y == label].count() for label in y.unique()\n",
    "}\n",
    "\n",
    "# Create train (80%), validation (10%), and test (10%) sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X_, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "X_ = {\n",
    "    'train': X_train,\n",
    "    'val': X_val,\n",
    "    'test': X_test\n",
    "}\n",
    "X_features = {\n",
    "    'train': get_features(X_train),\n",
    "    'val': get_features(X_val),\n",
    "    'test': get_features(X_test)\n",
    "}\n",
    "y = {\n",
    "    'train': y_train.astype(float),\n",
    "    'val': y_val.astype(float),\n",
    "    'test': y_test.astype(float)\n",
    "}\n",
    "\n",
    "print(\"Training size:\", X_['train'].shape[0])\n",
    "print(\"Validation size:\", X_['val'].shape[0])\n",
    "print(\"Test size:\", X_['test'].shape[0])\n",
    "print(\"Number of documents for each class:\", documents_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Try opening the file with the text already processed\n",
    "# otherwise process the text (it takes a while, that's why it's better to save it in a file)\n",
    "try:\n",
    "    f = open(X_pkl_file, 'rb')\n",
    "    X = pickle.load(f)\n",
    "    f.close()\n",
    "except FileNotFoundError:\n",
    "    X = {}\n",
    "    for part, documents in X_.items(): # part = ['train', 'val', 'test']\n",
    "        X[part] = []\n",
    "        for document in tqdm(documents):\n",
    "            # Lowercase (features like number of uppercase have already been computed)\n",
    "            document = document.lower()\n",
    "            \n",
    "            # Get the lemma of each word\n",
    "            document = [word.lemma_ for word in nlp(document)]\n",
    "            \n",
    "            # Put again words together in a text\n",
    "            document = ' '.join(document)\n",
    "            \n",
    "            # Append the document to X\n",
    "            X[part].append(document)\n",
    "        \n",
    "        # One hot encoding of the words\n",
    "        one_hot_rep = [one_hot(document, VOCABULARY_SIZE) for document in X[part]]\n",
    "        \n",
    "        # Pad if the text has less than WORDS_IN_SENTENCE words\n",
    "        X[part] = pad_sequences(one_hot_rep, padding='pre', maxlen=WORDS_IN_SENTENCE)\n",
    "\n",
    "        # Save everything in a file to avoid processing again next time\n",
    "        f = open(X_pkl_file, 'wb')\n",
    "        pickle.dump(X, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def build_nn(params):\n",
    "    \"\"\"\n",
    "    Function to build a feed-forward neural network using tf.keras.Functional model.\n",
    "\n",
    "    Arguments:\n",
    "    params (dict): A dictionary containing the following parameter data:\n",
    "                    features (list of strings): The features to use\n",
    "                    loss (string): The type of loss to optimize ('binary_crossentropy' or 'mse)\n",
    "                    optimizer (string): The type of optimizer to use while training ('sgd' or 'adam')\n",
    "                    epochs (int): The number of epochs\n",
    "\n",
    "    Returns:\n",
    "    model (tf.keras.Functional), a compiled model created using the specified parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Using a functional model to allow multiple inputs: https://www.tensorflow.org/guide/keras/functional\n",
    "    \n",
    "    # Inputs\n",
    "    input_text = Input(shape=(WORDS_IN_SENTENCE,))\n",
    "    input_features = Input(shape=(len(params['features']),))\n",
    "    \n",
    "    # Text part\n",
    "    embed = Embedding(VOCABULARY_SIZE, NUM_FEATURES_VECTOR, input_length=WORDS_IN_SENTENCE)(input_text)\n",
    "    lstm = LSTM(300, dropout=0.3, recurrent_dropout=0.3)(embed)\n",
    "    \n",
    "    # Concatenation\n",
    "    if len(params['features']) == 0:\n",
    "        conc = lstm\n",
    "    else:\n",
    "        # Features part\n",
    "        dense_features_1 = Dense(10, activation='relu')(input_features)\n",
    "        dense_features_2 = Dense(10, activation='relu')(dense_features_1)\n",
    "        \n",
    "        conc = Concatenate()([lstm, dense_features_2])\n",
    "    \n",
    "    # Final part\n",
    "    drop = Dropout(0.3)(conc)\n",
    "    dense = Dense(1)(drop)\n",
    "    activation = Activation('sigmoid')(dense)\n",
    "\n",
    "    # Model\n",
    "    model = Model([input_text, input_features], activation)\n",
    "    model.compile(loss=params['loss'], optimizer=params['optimizer'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model + tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot model\n",
    "\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# tokenizer = Tokenizer(num_words=5000)\n",
    "# embedding_matrix = np.zeros((30, 100))\n",
    "# for word, index in tokenizer.word_index.items():\n",
    "#     embedding_vector = embeddings_dictionary.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[index] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# model = build_nn({\n",
    "#     'loss': 'binary_crossentropy',\n",
    "#     'optimizer': 'adam',\n",
    "#     'epochs': 5,\n",
    "#     'features': ['percentage_punctuation', 'percentage_uppercase']\n",
    "# })\n",
    "# plot_model(model, to_file='model_2_features.pdf', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epochs': 5, 'loss': 'binary_crossentropy', 'optimizer': 'adam', 'features': []}\n",
      "Current combination of features and parameters already exists\n",
      "{'epochs': 10, 'loss': 'binary_crossentropy', 'optimizer': 'adam', 'features': []}\n",
      "Current combination of features and parameters already exists\n",
      "{'epochs': 20, 'loss': 'binary_crossentropy', 'optimizer': 'adam', 'features': []}\n",
      "Current combination of features and parameters already exists\n",
      "{'epochs': 5, 'loss': 'binary_crossentropy', 'optimizer': 'adam', 'features': ['percentage_punctuation']}\n",
      "Current combination of features and parameters already exists\n",
      "{'epochs': 10, 'loss': 'binary_crossentropy', 'optimizer': 'adam', 'features': ['percentage_punctuation']}\n",
      "Current combination of features and parameters already exists\n",
      "{'epochs': 20, 'loss': 'binary_crossentropy', 'optimizer': 'adam', 'features': ['percentage_punctuation']}\n",
      "Current combination of features and parameters already exists\n",
      "{'epochs': 5, 'loss': 'binary_crossentropy', 'optimizer': 'adam', 'features': ['percentage_uppercase']}\n",
      "Current combination of features and parameters already exists\n",
      "{'epochs': 10, 'loss': 'binary_crossentropy', 'optimizer': 'adam', 'features': ['percentage_uppercase']}\n",
      "Current combination of features and parameters already exists\n",
      "{'epochs': 20, 'loss': 'binary_crossentropy', 'optimizer': 'adam', 'features': ['percentage_uppercase']}\n",
      "Current combination of features and parameters already exists\n",
      "{'epochs': 5, 'loss': 'binary_crossentropy', 'optimizer': 'adam', 'features': ['percentage_punctuation', 'percentage_uppercase']}\n",
      "Current combination of features and parameters already exists\n",
      "{'epochs': 10, 'loss': 'binary_crossentropy', 'optimizer': 'adam', 'features': ['percentage_punctuation', 'percentage_uppercase']}\n",
      "Current combination of features and parameters already exists\n",
      "{'epochs': 20, 'loss': 'binary_crossentropy', 'optimizer': 'adam', 'features': ['percentage_punctuation', 'percentage_uppercase']}\n",
      "Current combination of features and parameters already exists\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from time import time\n",
    "from utils import reset_seeds\n",
    "\n",
    "# df_results contains already done tests to avoid training again the network\n",
    "# for something that has already been computed\n",
    "try:\n",
    "    df_results = pd.read_csv(TESTS_FILENAME)\n",
    "except FileNotFoundError:\n",
    "    df_results = pd.DataFrame(columns=['features', 'loss', 'optimizer', 'epochs', 'filename'])\n",
    "\n",
    "param_grid = {\n",
    "    'loss': ['binary_crossentropy'],\n",
    "    'optimizer': ['adam'],\n",
    "    'epochs': [5, 10, 20]\n",
    "}\n",
    "\n",
    "for features in [\n",
    "                 [],\n",
    "                 ['percentage_punctuation'],\n",
    "                 ['percentage_uppercase'],\n",
    "                 ['percentage_punctuation', 'percentage_uppercase']\n",
    "                ]:\n",
    "    results = []\n",
    "    best_model = None\n",
    "    best_model_value = None\n",
    "\n",
    "    for parameters in ParameterGrid(param_grid):\n",
    "        parameters['features'] = features\n",
    "        print(parameters)\n",
    "\n",
    "        # Check if already evaluated this combination\n",
    "        if len(features) == 0:\n",
    "            df_features_mask = ((df_results['features'] == '') | (df_results['features'].isnull()))\n",
    "        else:\n",
    "            df_features_mask = ((df_results['features'] == ','.join(features)))\n",
    "\n",
    "        # If so, then skip this training\n",
    "        if df_results[\n",
    "            df_features_mask &\n",
    "            (df_results['loss'] == parameters['loss']) &\n",
    "            (df_results['optimizer'] == parameters['optimizer']) &\n",
    "            (df_results['epochs'] == parameters['epochs'])\n",
    "        ].shape[0] > 0:\n",
    "            print(\"Current combination of features and parameters already exists\")\n",
    "            continue\n",
    "\n",
    "        # Build the network\n",
    "        reset_seeds()\n",
    "        model = build_nn(parameters)\n",
    "\n",
    "        # Train the network\n",
    "        reset_seeds()\n",
    "        input_model_fit = X['train'] if len(features) == 0 else [X['train'], X_features['train'][features]]\n",
    "        model.fit(input_model_fit, y['train'], epochs=parameters['epochs'])\n",
    "        \n",
    "        # Evaluate the network on validation set\n",
    "        input_model_predict = X['val'] if len(features) == 0 else [X['val'], X_features['val'][features]]\n",
    "        y_pred = model.predict(input_model_predict)\n",
    "        y_pred = [1 if prob >= 0.5 else 0 for prob in y_pred]\n",
    "\n",
    "        # Save predictions and true values on a file\n",
    "        current_time = str(time())\n",
    "        results_filename = ABSOLUTE_PATH + './results/results_'+ current_time +'.csv'\n",
    "        pd.DataFrame({'true':y['val'], 'pred':y_pred}).to_csv(results_filename, index=False)\n",
    "\n",
    "        # Add this result to the dataframe and update the file containing all the hyperparameters tested\n",
    "        df_results = df_results.append([{\n",
    "            'features': ','.join(features),\n",
    "            'loss': parameters['loss'],\n",
    "            'optimizer': parameters['optimizer'],\n",
    "            'epochs': parameters['epochs'],\n",
    "            'filename': results_filename,\n",
    "        }])\n",
    "        df_results.to_csv(TESTS_FILENAME, index=False)\n",
    "\n",
    "        # Save model to file\n",
    "        model.save(ABSOLUTE_PATH + './results/models/'+current_time+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the best model on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found the best hyperparameters among the ones we tried: great! Now we take the hyperparameters that performed the best on the validation set and we use the test set to evaluate our model. If the metric are widely different from the ones we got with the evaluation set, then we have to be careful because it's very likely that we overfitted on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: []\n",
      "Accuracy: 0.88\n",
      "Precision: [0.87931034 0.88095238]\n",
      "Recall: [0.91071429 0.84090909]\n",
      "F1: [0.89473684 0.86046512]\n",
      "Parameters: ['percentage_punctuation']\n",
      "Accuracy: 0.86\n",
      "Precision: [0.83870968 0.89473684]\n",
      "Recall: [0.92857143 0.77272727]\n",
      "F1: [0.88135593 0.82926829]\n",
      "Parameters: ['percentage_uppercase']\n",
      "Accuracy: 0.9\n",
      "Precision: [0.88333333 0.925     ]\n",
      "Recall: [0.94642857 0.84090909]\n",
      "F1: [0.9137931  0.88095238]\n",
      "Parameters: ['percentage_punctuation', 'percentage_uppercase']\n",
      "Accuracy: 0.84\n",
      "Precision: [0.82258065 0.86842105]\n",
      "Recall: [0.91071429 0.75      ]\n",
      "F1: [0.86440678 0.80487805]\n"
     ]
    }
   ],
   "source": [
    "best_models = [\n",
    "    {\n",
    "        'features': [],\n",
    "        'time': '1607114853.1603158',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'optimizer': 'adam',\n",
    "        'epochs': 20\n",
    "    },\n",
    "    {\n",
    "        'features': ['percentage_punctuation'],\n",
    "        'time': '1607117869.2251015',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'optimizer': 'adam',\n",
    "        'epochs': 10\n",
    "    },\n",
    "    {\n",
    "        'features': ['percentage_uppercase'],\n",
    "        'time': '1607169573.7067382',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'optimizer': 'adam',\n",
    "        'epochs': 20\n",
    "    },\n",
    "    {\n",
    "        'features': ['percentage_punctuation','percentage_uppercase'],\n",
    "        'time': '1607178480.4414513',\n",
    "        'loss': 'binary_crossentropy',\n",
    "        'optimizer': 'adam',\n",
    "        'epochs': 20\n",
    "    },\n",
    "]\n",
    "\n",
    "for parameters in best_models:\n",
    "    print(\"Parameters:\", parameters['features'])\n",
    "    \n",
    "    try:\n",
    "        model = tf.keras.models.load_model(ABSOLUTE_PATH + './results/models/' + parameters['time'] + '.h5')\n",
    "\n",
    "    # If model has not been saved\n",
    "    except OSError:\n",
    "        # Build the model\n",
    "        reset_seeds()\n",
    "        model = build_nn(parameters)\n",
    "        \n",
    "        # Train the model\n",
    "        reset_seeds()\n",
    "        input_model_fit = X['train'] if len(parameters['features']) == 0 else [X['train'], X_features['train'][parameters['features']]]\n",
    "        model.fit(input_model_fit, y['train'], epochs=parameters['epochs'])\n",
    "\n",
    "        # Save the trained model\n",
    "        model.save(ABSOLUTE_PATH + './results/models/' + parameters['time'] + '.h5')\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    input_model_predict = X['test'] if len(parameters['features']) == 0 else [X['test'], X_features['test'][parameters['features']]]\n",
    "    y_pred = model.predict(input_model_predict)\n",
    "    y_pred = [1 if prob >= 0.5 else 0 for prob in y_pred]\n",
    "\n",
    "    # Compute accuracy, precision, recall, f1\n",
    "    a = accuracy_score(y['test'], y_pred)\n",
    "    p, r, f1, s = precision_recall_fscore_support(y['test'], y_pred)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(\"Accuracy:\", a)\n",
    "    print(\"Precision:\", p)\n",
    "    print(\"Recall:\", r)\n",
    "    print(\"F1:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
